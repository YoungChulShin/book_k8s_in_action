# 쿠버네티스 내부 이해
## 아키텍처 이해
컨트롤 플레인 구성 요소
- etcd 분산 저장 스토리지
- API 서버
- 스케쥴러
- 컨트롤러 매니저
- `k get componentstatuses` 명령어로 상태를 확인할 수 있다
   ```
   NAME                 STATUS      MESSAGE                                                                                       ERROR
   scheduler            Unhealthy   Get "http://127.0.0.1:10251/healthz": dial tcp 127.0.0.1:10251: connect: connection refused
   controller-manager   Healthy     ok
   etcd-0               Healthy     {"health":"true","reason":""}
   ```

워커 노드에서 구성 요소
- Kubelet
- 쿠버네티스 서비스 프록시
- 컨테이너 란타임

애드온 구성 요소
- DNS 서버
- 대시보드
- Ingress Controller
- 힙스터
- 컨테이너 네트워크 인터페이스

구성 요소 통신
- API 서버하고만 통신한다. 직접 통신하지 않는다.

개발 구성 요소의 여러 인스턴스 실행
- 워커 노드의 구성요소는 모두 동일한 노드에서 실행돼야 한다
- 컨트롤 플레인 구성요소는 여러 서버에 걸쳐 실행될 수 있는데, 들 이상 실행해 가용성을 높일 수 있다
   - etcd, API 서버: 병렬로 수행 가능하다
   - 스케쥴러, 컨트롤러 매니저: 하나의 인스턴스만 활성화되고 나머지는 대기 상태에 있는다

구성 요소 실행 방법
- Kubelet은 일반 시스템 구성요소로 실행되는 유일한 구성 요소이며, Kublet이 다른 구성 요소를 파드로 실행한다
- 컨트롤 플레인 구성요소를 파드로 실행하기 위해서 Kubelet도 마스터 노드에 배포된다
- 구성 요소 확인
   ```
    // k get pod -o custom-columns=POD:metadata.name,NODE:spec.nodeName --sort-by spec.nodeName -n kube-system
    // etcd, api server, core-dns, schduler, controller-manager는 마스터에서 실행
    // kube-proxy, kindnet은 워커 노드에서 실행
    POD                                      NODE
    kube-apiserver-multinode-demo            multinode-demo
    etcd-multinode-demo                      multinode-demo
    kube-scheduler-multinode-demo            multinode-demo
    kube-proxy-wp5rf                         multinode-demo
    kube-controller-manager-multinode-demo   multinode-demo
    kindnet-vwszg                            multinode-demo
    coredns-78fcd69978-z45rr                 multinode-demo
    storage-provisioner                      multinode-demo
    kube-proxy-wfwn9                         multinode-demo-m02
    kindnet-dlbzf                            multinode-demo-m02
    kube-proxy-g4sdl                         multinode-demo-m03
    kindnet-j8j6l                            multinode-demo-m03
    kube-proxy-2xzrd                         multinode-demo-m04
    kindnet-75ps4                            multinode-demo-m04
   ```

etcd
- 매니패스트(파드, RC, 서비스, 시크릿 등) 정보가 영구적으로 저장되는 저장소
- 빠르고, 분산해서 저장되며, 일관된 키-값 저장소
- API 서버만 etcd와 직접적으로 통신한다
- 오브젝트의 일관성과 유효성 보장
   - API 서버를 통해서만 통신하도록 함으로써 일관성(낙관적 잠금)을 제공한다
   - 2개 이상의 ectd 인스턴스는 RAFT 합의 알고리즘을 사용해서 각 노드의 상태가 대다수의 노드가 동의하는 현재 상태이거나 이전에 동의된 상태 중에 하나님을 보장한다.
   - 합의 알고리즘 때문에 보통 홀 수로 구성을 한다. 
      - 예: 3개, 4개 모두 1개의 인스턴스의 실패만 허용한다. 4개는 과반을 위해서 3개가 필요한데, 1개만 실패도 3개가 되기 때문이다

API 서버
- 다른 모든 구성요소와 kubelet 같은 클라이언트에서 사용하는 중심 구성 요소
- 클러스터 상태를 조회/변경하기 위해 RESTful API로 CRUD 인터페이스를 제공한다. 상태를 etcd에 저장된다.
- 낙관적 잠금으로 일관성을 보장하고, 유효성 검사로 유효성을 보장한다
- API 서버의 동작
   1. kubectl로 요청 (예: 리소스 생성)
   2. POST 요청
   3. 인증 플러그인 통과
   4. 인가 플러그인 통과
   5. 어드미션 컨트롤 플러그인 통과
   6. 리소스 유효성 검사
   7. etcd에 저장
- 리소스 변경 및 통보
   - 클라이언트는 API 서버에 HTTP 연결을 맺고 변경 사항을 감지한다. 클라이언트는 오브젝트의 변경을 알 수 있는 스트림을 받는다. 
   - 오브젝트가 갱신되면 API 서버는 클라이언트에게 새로운 버전을 보낸다
   - 예시
      ```
      // kubectl로 모니터링 실행
      k get pods --watch

      // 확인
      NAME      READY   STATUS    RESTARTS   AGE
      kubia-0   1/1     Running   0          42m
      kubia-1   1/1     Running   0          42m    // 기존 상태
      kubia-1   1/1     Terminating   0          43m    // statefulset 삭제
      kubia-0   1/1     Terminating   0          43m
      kubia-0   0/1     Terminating   0          43m
      kubia-0   0/1     Terminating   0          43m
      ```

스케줄러
- 역할
   - API 서버의 감시 매커니즘을 통해 새로 생성될 파드를 기다리고 있다가 할당된 노드가 없는 새로운 파드를 노드에 할당한다. (이때문에 일반적으로 파드를 생성할 때 노드를 지정하는 않는다)
   - 선택된 노드에 파드를 실행하도록 지시하지는 않고, API 서버로 파드의 정의를 갱신한다. API 서버를 통해서 Kublet이 이를 통보 받고 파드의 컨테이너를 생성하고 실행한다. 
- 스케쥴링 알고리즘
   1. 모든 노드 중에서 파드를 스케줄링 할 수 있는 노드 목록을 필터링한다
   2. 수용 가능한 노드의 우선순위를 정하고 점수가 높은 노드를 선택한다. 최상위 점수의 노드가 여러개 있다면 라운드-로빈 방식을 사용해서 고르게 배포되도록 한다. 
- 다중 스케줄러
   - 스케줄러는 여러개 실행할 수 있다
   - 파드 정의안의 'schedulerName' 속성에 스케줄러를 지정할 수 있고, 이 속성을 사용 안하면 기본 스케줄러를 사용한다

컨트롤러 매니저
- 컨트롤러 매니저의 다양한 컨트롤러가 리소스를 처리한다
- 컨트롤로 목록: 여러 컨트롤러가 하나의 컨트롤러 매니저 프로세스에서 실행된다
   - 레플리케이션 매니저
   - 레플리카 셋, 데몬 셋, 잡 컨트롤러
      - 파드 셀렉터와 일치하는 파드의 수를 찾고, 이를 원하는 레프리카 수(레플리카셋 리소스로부터)와 비교한다
      - 숫자가 너무 작으면 새로운 파드 매니패스트를 생성해서 API 서버에 게시하고, 스케줄러와 Kubelet이 파드 스케줄링과 실행 작업을 수행한다
         1. 스케줄러가 파드 리소스에 노드 정보를 업데이트
         2. Kubelet이 생성 진행
   - 디플로이먼트 컨트롤러
      - 실제 배포 상태와 디플로이먼트 API 오브젝트에 기록된 원하는 상태가 동기화되도록 관리한다
      - 디플로이먼트 오브젝트가 수정될 때마다(수정이 파드에 영향을 준다면) 새로운 버전을 롤아웃한다 
   - 스테이트풀셋 컨트롤러
      - 각 파드를 위한 PVC도 인스턴스화 하고 관리한다. (정의 PVC이 포함되어 있다)
   - 노드 컨트롤러
      - 클러스터에서 실행 중인 실제 머신 목록과 노드 오브젝트 목록을 동기화하는 데 중점을 둔다
      - 각 노드의 상태를 모니터링하고 연결이 끊어진 노드에서 파드를 제거한다
   - 서비스 컨트롤러
      - LoadBalancer 유형의 서비스가 생성되거나 삭제될 때 인프라스트럭쳐에 로드 밸런서를 요청하고 해제하는 역할
   - 엔드포인트 컨트롤러
      - 레이블 셀렉터와 일치하는 파드의 IP와 포트로 엔드포인트 리스트를 계속 갱신하는 활성 구성 요소
      - 서비스와 파드 리소스를 모두 감시한다. 
   - 네임스페이스 컨트롤러
      - 네임스페이스 오브젝트의 삭제 토옵를 받으면 속해 있는 모든 리소스를 삭제한다
   - 퍼시스턴트볼륨 컨트롤러 등
- 리소스는 클러스터에 어떤 것을 실행해야하는지 기술하는 반면, 컨트롤러는 리소스를 배포함에 따라 실제 작업을 수행하는 활성화된 쿠버네티스 구성요소
- 역할
   - API 서버에서 리소스가 변경되는 것을 감지하고 변경 작업을 수행한다. 보통 리소스 생성, 리소스 자체 갱신 등이 포함된다
   - 감시 매커니즘을 이용해서 변경 사항을 통보받지만 모든 이벤트를 놓치지 않고 받는다는 것을 보장하지는 않기 때문에 정기적으로 목록을 가져오는 작업을 수행해 누락된 이벤트가 없는지 확인해야한다.
- 모든 컨트롤러는 API 서버로 API 오브젝트를 제어한다. Kubelet과 직접 통신하지 않는다

Kublet
- 역할
   - 워커노드에서 실행하는 모든 것을 담당하는 구성 요소
   - 실행중인 노드를 노드 리소스로 만들어서 API 서버에 등록한다
   - 노드에 파드가 스케줄링되면 파드의 컨테이너를 시작한다
   - 실행 중인 컨테이너를 계속 모니터링하면서 상태, 이벤트, 리소스 사용량을 API 서버에 보고한다
   - 컨테이너 라이브니스 프로브를 실행하는 구성 요소. 프로브가 실패할 경우 컨테이너를 다시 시작한다
   - API 서버에서 파드가 삭제되면 컨테이너를 정지하고 파드가 종료된 것을 서버에 통보한다
- API 서버와 통신해서 파드 매니페스트를 가져오지만, 워커 노드의 특정 디렉토리 안에 있는 매니페스트 파일을 기반으로 파드를 시작할 수 있다.
   - 이 기능은 컨트롤 플레인 구성 요소를 파드로 실행하는데 사용된다

서비스 프록시
- 역할
   - 클라이언트가 쿠버네티스 API로 정의한 서비스에 연결할 수 있도록 해준다
   - 서비스의 IP와 포트로 들어온 접속을 서비스를 지원하는 파드 중 하나와 연결시켜준다
- iptables 프록시 모드
   - iptables 규칙만을 사용해 프록시 서버를 거치지 않고 파드로 전달한다
   - kube-proxy가 iptables를 구성
   - kube-proxy를 통해서 pod로 전달되지 않기 때문에(커널 모드) 성능이 더 좋다

애드온
- DNS 서버
   - core-dns 서비스로 노출
   - 서비스 IP는 /etc/resolve.conf 파일안에 nameserver로 지정돼 있다
   - API서버 감시 매커니즘을 통해서 서비스와 엔드포인트 변화를 관찰하고 모든 변화를 DNS 레코드에 갱신한다
- 인그레스 컨트롤러
   - 리버스 프록시(예: NginX)를 실행하고 클러스터에 정의된 인그레스, 서비스, 엔드포인트 리소스 설정을 유지한다
   - 클라이언트는 이러한 리소스를 감시하고 변경이 일어날 때마다 프록시 서버를 설정을 변경한다

컨트롤러가 협업하는 방법: 디플로이먼트를 생성하는 예시
1. kubectl이 매니페스트를 POST 요청으로 API 서버에 전송한다
2. API 서버는 디플로먼트 정의를 검증하고 etcd에 저장한후 kubectl에 응답을 돌려준다.
   - 디플로이먼트 리소스 생성
3. 디플로이먼트 컨트롤러가 디플로이먼트 리소스가 생성된 통보를 받는다
4. 디플로이먼트 컨트롤러가 레플리카셋 리소스를 생성
5. 레플리카셋 컨트롤러가 레플리카셋 리소스가 생성된 통보를 받는다
6. 레플리카셋 컨트롤러는 레플리카셋에 정의된 복제본 수와 파드 셀렉터와 일치하는 실행중인 파드수를 비교한다
7. 레플리카셋 컨트롤러가 필요할 경우 파드 리소스를 생성한다
8. 파드가 생성되면 스케줄러가 통보를 받는다
9. 스케줄러가 적합한 노드를 선정해서 파드를 노드에 할당한다
10. Kubelectl은 노드에 파드가 스케줄링된 것을 통보 받는다
11. Kubelectl은 컨테이너 런타임에 컨테이너를 시작하도록 지시한다. 
12. 컨테이너 런타임이 컨테이너를 시작한다

실행중인 파드에서의 컨테이너
- 컨테이너 노드에서 'docker ps'로 컨테이너 리스트를 보면 실행한 컨테이너 외에 'pause' 컨테이너를 확인할 수 있다
- 퍼즈 컨테이너는 파드의 모든 컨테이너를 담고 있는 컨테이너이다. 네임스페이스를 보유하는 게 유일한 목적인 인프라스트럭처 컨테이너이다.
   - 이때문에 파드의 모든 컨테이너는 동일한 네트워크와 리녹스 네임스페이스를 공유한다

파드간 네트워킹
- 파드가 동일 워커노드에서 실행 중인지 여부와 상관없이 파드끼리 서로 통신할 수 있어야한다
- NAT없이 플랫네트워크로 서로 통신한다. 예: 파드A에서 파드B로 연결할 때 파드B가 보는 출발지 IP는 파드A의 IP와 동일해야한다
- 파드가 인터넷에 있는 서비스와 통신할 때에는 출발지 IP를 ㅂ녀경하는 것이 필요하다. 호스트워커 노드의 IP로 변경된다. 

네트워킹 동작 방식
- 기본 연결 구조
   - 파드 안의 eth0 네트워크 <-> 가상 인터페이스 쌍(veth) <-> Bridge
- 동일한 노드에서 파드간의 통신
   - 파드 A에서 파드 B로 보내느 경우는 아래와 같다
      1. 파드 A의 veth 쌍을 통해서 브리지로 전달된다
      2. 브리지를 통해서 파드 B의 veth 쌍을 통과한다
- 서로 다른 노드에서 파드간의 통신
   - 파드의 IP주소는 전체 클러스터에 유일해야하기 때문에 노드 사이의 브리지는 겹치지 않는 주소 범위를 사용해야한다
   - 브리지는 물리네트워크 어댑터에 연결된다

서비스 
- 역할: 파드 집합을 길제 지속되는 안정적인 IP와 포트로 노출시킨다
- 서비스가 생성될 때의 과정
   1. API 서버에서 파드를 서비스를 생성하면 가상 IP 주소가 할당된다
   2. API 서버는 워커 노드의 kube-proxy에 새로운 서비스가 생성된 것을 통보한다
   3. kube-proxy는 iptable를 업데이트해서 실행중인 노드에서 해당 서비스 주소로 접속할 수 있게 해준다
      - kube-proxy는 엔드포인트도 감시한다
- 파드에서 다른 파드로 요청할 때의 과정
   1. 파드 A에서 파드 B로 가기 위한 패킷을 생성한다. 
      - 여기서 목적지는 서비스의 IP이다
   2. 패킷이 네트워크로 전송되기 전에 iptable에 설정된 규칙에 따라 처리된다
      - iptable에 파드 B의 ip가 있기 때문에 도착지 IP가 변경된다

고가용성 클러스터 실행
- 애플리케이션 가용성 높이기
   - 쿠버네티스의 다양한 컨트롤러는 노드에 장애가 발생해도 애플리케이션이 특정 규모로 원활하게 동작할 수 있게 해준다
   - 디플로이먼트로 애플리케이션을 실행하고 적절한 수의 레플리카 수를 설정한다
- 가동 중단 시간을 줄이기 위한 다중 인스턴스 실행
   - 애플리케이션이 수평 확장을 할 수 없더라도 레플리카 수를 1로 지정된 디플로이먼트를 사용해야한다. 
- 컨트롤 플레인 구성 요소의 가용성
   - etcd
      - 분산시스템으로 설계됐으므로 가용성을 높이는 것은 큰 문제가 되지 않는다
      - 필요한 수의 머신에서 서로를 인식할 수 있게 하면 된다
      - 모든 인스턴스에 걸쳐 데이터를 복제하기 때문에 세 대의 머신으로 구성된 클러스터는 한 노드가 실패하더라도 나머지 2개로 읽기와 쓰기를 할 수 있다
   - API 서버 인스턴스
      - API 서버는 상태를 저장하지 않기 때문에 필요한 만큼 서버를 실행할 수 있고 서로를 인지할 필요가 없다.
      - 일반적으로 etcd 인스턴스테 API 서버를 함께 띄운다. 이렇게 하면 etcd 인스턴스 앞에 로드밸런서를 둘 필요가 없다.
      - API 서버는 로드밸런서가 앞에 위치한다
   - 컨트롤러, 스케줄러
      - 컨트롤러와 스케줄러는 클러스터 상태를 감시하고 상태가 변경될 때 반응해야 하기 때문에 여러 인스턴스가 동시에 수행되는 것을 처리하기가 어렵다.
      - 따라서 한번에 1개의 인스턴스만 활성화 되고 나머지는 대기한다. 

궁금한것
- 494P 